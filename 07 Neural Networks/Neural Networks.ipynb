{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "In this exercise, several parts of the code are missing, which should be completed by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Skin disease dataset using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us apply a neural network on the skin disesase data. To reduce the training time we reduce the amount of data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321097</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.803696</td>\n",
       "      <td>146.523499</td>\n",
       "      <td>148.061279</td>\n",
       "      <td>2.687050</td>\n",
       "      <td>2.307853</td>\n",
       "      <td>11.867569</td>\n",
       "      <td>2.337752</td>\n",
       "      <td>1.718121</td>\n",
       "      <td>-7.890529</td>\n",
       "      <td>3.511095</td>\n",
       "      <td>2.936168</td>\n",
       "      <td>14.708332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369319</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.726944</td>\n",
       "      <td>142.700180</td>\n",
       "      <td>135.994049</td>\n",
       "      <td>1.718374</td>\n",
       "      <td>1.884356</td>\n",
       "      <td>9.526067</td>\n",
       "      <td>1.167531</td>\n",
       "      <td>-1.294433</td>\n",
       "      <td>9.130569</td>\n",
       "      <td>2.348968</td>\n",
       "      <td>2.505262</td>\n",
       "      <td>12.324208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257867</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.221893</td>\n",
       "      <td>144.206421</td>\n",
       "      <td>132.246872</td>\n",
       "      <td>1.454083</td>\n",
       "      <td>2.776567</td>\n",
       "      <td>7.053739</td>\n",
       "      <td>0.315222</td>\n",
       "      <td>-2.041615</td>\n",
       "      <td>5.701283</td>\n",
       "      <td>1.778443</td>\n",
       "      <td>3.617439</td>\n",
       "      <td>9.583660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.394958</td>\n",
       "      <td>137.128250</td>\n",
       "      <td>108.684502</td>\n",
       "      <td>1.080449</td>\n",
       "      <td>0.934561</td>\n",
       "      <td>12.131411</td>\n",
       "      <td>-1.053084</td>\n",
       "      <td>-0.607379</td>\n",
       "      <td>-5.108250</td>\n",
       "      <td>1.576523</td>\n",
       "      <td>1.206285</td>\n",
       "      <td>14.707682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477050</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.810623</td>\n",
       "      <td>142.992310</td>\n",
       "      <td>125.166321</td>\n",
       "      <td>1.168441</td>\n",
       "      <td>1.926482</td>\n",
       "      <td>15.939886</td>\n",
       "      <td>-0.953198</td>\n",
       "      <td>-0.629208</td>\n",
       "      <td>-12.651587</td>\n",
       "      <td>1.599050</td>\n",
       "      <td>2.570897</td>\n",
       "      <td>19.383097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class   t0   t1          t2          t3          t4        t5  \\\n",
       "321097      1  0.0  1.0  146.803696  146.523499  148.061279  2.687050   \n",
       "369319      0  0.0  1.0  147.726944  142.700180  135.994049  1.718374   \n",
       "257867      1  0.0  1.0  146.221893  144.206421  132.246872  1.454083   \n",
       "6870        0  0.0  1.0  148.394958  137.128250  108.684502  1.080449   \n",
       "477050      0  0.0  1.0  147.810623  142.992310  125.166321  1.168441   \n",
       "\n",
       "              t6         t7        t8        t9        t10       t11  \\\n",
       "321097  2.307853  11.867569  2.337752  1.718121  -7.890529  3.511095   \n",
       "369319  1.884356   9.526067  1.167531 -1.294433   9.130569  2.348968   \n",
       "257867  2.776567   7.053739  0.315222 -2.041615   5.701283  1.778443   \n",
       "6870    0.934561  12.131411 -1.053084 -0.607379  -5.108250  1.576523   \n",
       "477050  1.926482  15.939886 -0.953198 -0.629208 -12.651587  1.599050   \n",
       "\n",
       "             t12        t13  \n",
       "321097  2.936168  14.708332  \n",
       "369319  2.505262  12.324208  \n",
       "257867  3.617439   9.583660  \n",
       "6870    1.206285  14.707682  \n",
       "477050  2.570897  19.383097  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"skin_disease.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df = df.iloc[0:100000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split the data into a train and test set. Use 40% of the data for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our Multi Layer Perceptron with 2 hidden layers. This time we use the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) implementation from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,4),\n",
    "                    activation='relu',  # activation function\n",
    "                    solver='adam',  # optimizer\n",
    "                    batch_size=1024)  # size of minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the neural network on `X_train`, `y_train` and plot the loss by accessing the attribute `loss_curve_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(costs)\n",
    "    ax.set_title(\"Loss curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "plot_costs(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement your own predict function. For that we'll need an activation function for the hidden layers, in our case `relu` and for the output layer `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    ### START YOUR CODE ###\n",
    "    return np.max(0,x)\n",
    "    ### END YOUR CODE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### START YOUR CODE ###\n",
    "    sig = 1 / ( 1+ np.exp(-x))\n",
    "    ### END YOUR CODE ###\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(mlp, X):\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    # define the first activations, e.g. inputs\n",
    "    \n",
    "    # forward propagate through layers\n",
    "            \n",
    "    # last layer output activation `sigmoid`\n",
    "    \n",
    "    # transform to 1-D and threshold\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def predict(mlp, X):\n",
    "    # define the first activations, e.g. inputs\n",
    "    A = X\n",
    "    \n",
    "    # forward propagate through layers\n",
    "    for i, (W, B) in enumerate(zip(mlp.coefs_, mlp.intercepts_)):\n",
    "        z = A.dot(W) + B\n",
    "        # if hidden layer, apply `relu`\n",
    "        if i != mlp.n_layers_ - 2:\n",
    "            A = relu(z)\n",
    "            \n",
    "    # last layer output activation `sigmoid`\n",
    "    out = sigmoid(z)  \n",
    "    # transform to 1-D and threshold\n",
    "    out = np.squeeze(out)\n",
    "    out = np.array(out > 0.5, dtype=int)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test your implementation with the scikit-learn predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the outputs the same: True\n"
     ]
    }
   ],
   "source": [
    "y_pred_scikit = mlp.predict(X_test)\n",
    "y_pred_own = predict(mlp,X_test)\n",
    "\n",
    "print('Are the outputs the same: %s' % (y_pred_scikit == y_pred_own).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the outputs the same: True\n"
     ]
    }
   ],
   "source": [
    "y_pred_scikit = mlp.predict(X_test)\n",
    "y_pred_own = predict(mlp, X_test.values)\n",
    "\n",
    "print('Are the outputs the same: %s' % (y_pred_scikit == y_pred_own).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Predict the values on the test set and calculate the accuracy and the f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9059\n",
      "F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(mlp, X_test.values)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Skin disease dataset using `TensorFlow`\n",
    "In practice, the MLP from `scikit-learn` is never used because of the lack of customisation and the absence of GPU training. `TensorFlow` is a library specialised in deep learning and therefore also has implementations for advanced techniques. Thus the section below is a quick introduction to how the same network can be implemented using `TensorFlow`. The networks' results do not need to be the same, since as mentioned above, the `scikit-learn` implementation can not be as customised as the `TensorFlow` one. \n",
    "\n",
    "If `TensorFlow` is not already installed on the environment, it can be done using the \"magic\" cell from below. If it is already installed, make sure to use version `2.3.1`. Higher versions should also work but weren't tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#assert tf.__version__ == '2.3.1'\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model in `TensorFlow` can be implemented using the `Sequential API`, which enables for easy extensibility by calling `.add()`. To implement the same MLP as above, we can sequentially add `Dense` layers to the model. Here the customization possibilities compared to `scikit-learn` is evident. For example, the activation function can be set for each layer separately, which was impossible before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                450       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                465       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 931\n",
      "Trainable params: 931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataset_dim = X_train.shape[1]\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(30, input_shape=(dataset_dim, ), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, it needs to be compiled using an optimizer and loss function. In our case, we'll use adam as optimizer and binary cross-entropy as loss. Now the model can be trained by specifying the number of epochs and the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "59/59 [==============================] - 1s 1ms/step - loss: 0.2897 - accuracy: 0.8982\n",
      "Epoch 2/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9125\n",
      "Epoch 3/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.2167 - accuracy: 0.9153\n",
      "Epoch 4/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9189\n",
      "Epoch 5/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9226\n",
      "Epoch 6/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9246\n",
      "Epoch 7/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9272\n",
      "Epoch 8/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1787 - accuracy: 0.9293\n",
      "Epoch 9/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9306\n",
      "Epoch 10/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9313\n",
      "Epoch 11/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9331\n",
      "Epoch 12/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9347\n",
      "Epoch 13/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9355\n",
      "Epoch 14/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9363\n",
      "Epoch 15/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9346\n",
      "Epoch 16/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1578 - accuracy: 0.9373\n",
      "Epoch 17/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1576 - accuracy: 0.9372\n",
      "Epoch 18/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1556 - accuracy: 0.9383\n",
      "Epoch 19/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1541 - accuracy: 0.9384\n",
      "Epoch 20/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1532 - accuracy: 0.9392\n",
      "Epoch 21/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1536 - accuracy: 0.9390\n",
      "Epoch 22/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1528 - accuracy: 0.9392\n",
      "Epoch 23/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9389\n",
      "Epoch 24/150\n",
      "59/59 [==============================] - 0s 983us/step - loss: 0.1527 - accuracy: 0.9391\n",
      "Epoch 25/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1506 - accuracy: 0.9405\n",
      "Epoch 26/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1506 - accuracy: 0.9404\n",
      "Epoch 27/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1493 - accuracy: 0.9413\n",
      "Epoch 28/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1501 - accuracy: 0.9402\n",
      "Epoch 29/150\n",
      "59/59 [==============================] - 0s 1000us/step - loss: 0.1514 - accuracy: 0.9399\n",
      "Epoch 30/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9412\n",
      "Epoch 31/150\n",
      "59/59 [==============================] - 0s 1000us/step - loss: 0.1478 - accuracy: 0.9419\n",
      "Epoch 32/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1505 - accuracy: 0.9405\n",
      "Epoch 33/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1490 - accuracy: 0.9408\n",
      "Epoch 34/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1469 - accuracy: 0.9419\n",
      "Epoch 35/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1473 - accuracy: 0.9420\n",
      "Epoch 36/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1459 - accuracy: 0.9424\n",
      "Epoch 37/150\n",
      "59/59 [==============================] - 0s 965us/step - loss: 0.1483 - accuracy: 0.9412\n",
      "Epoch 38/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1457 - accuracy: 0.9420\n",
      "Epoch 39/150\n",
      "59/59 [==============================] - 0s 965us/step - loss: 0.1448 - accuracy: 0.9424\n",
      "Epoch 40/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9428\n",
      "Epoch 41/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9423\n",
      "Epoch 42/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9430\n",
      "Epoch 43/150\n",
      "59/59 [==============================] - 0s 1000us/step - loss: 0.1435 - accuracy: 0.9433\n",
      "Epoch 44/150\n",
      "59/59 [==============================] - 0s 983us/step - loss: 0.1431 - accuracy: 0.9437\n",
      "Epoch 45/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1430 - accuracy: 0.9431\n",
      "Epoch 46/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1433 - accuracy: 0.9439\n",
      "Epoch 47/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1430 - accuracy: 0.9439\n",
      "Epoch 48/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1423 - accuracy: 0.9439\n",
      "Epoch 49/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1406 - accuracy: 0.9451\n",
      "Epoch 50/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1414 - accuracy: 0.9441\n",
      "Epoch 51/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1426 - accuracy: 0.9443\n",
      "Epoch 52/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1434 - accuracy: 0.9434\n",
      "Epoch 53/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1428 - accuracy: 0.9434\n",
      "Epoch 54/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1431 - accuracy: 0.9438\n",
      "Epoch 55/150\n",
      "59/59 [==============================] - 0s 965us/step - loss: 0.1399 - accuracy: 0.9453\n",
      "Epoch 56/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1403 - accuracy: 0.9449\n",
      "Epoch 57/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1401 - accuracy: 0.9451\n",
      "Epoch 58/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1411 - accuracy: 0.9438\n",
      "Epoch 59/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1406 - accuracy: 0.9448\n",
      "Epoch 60/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1394 - accuracy: 0.9457\n",
      "Epoch 61/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1390 - accuracy: 0.9452\n",
      "Epoch 62/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1396 - accuracy: 0.9449\n",
      "Epoch 63/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1392 - accuracy: 0.9451\n",
      "Epoch 64/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1407 - accuracy: 0.9444\n",
      "Epoch 65/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1381 - accuracy: 0.9457\n",
      "Epoch 66/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1380 - accuracy: 0.9457\n",
      "Epoch 67/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1366 - accuracy: 0.9461\n",
      "Epoch 68/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1380 - accuracy: 0.9457\n",
      "Epoch 69/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1397 - accuracy: 0.9446\n",
      "Epoch 70/150\n",
      "59/59 [==============================] - 0s 879us/step - loss: 0.1386 - accuracy: 0.9451\n",
      "Epoch 71/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1373 - accuracy: 0.9463\n",
      "Epoch 72/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1371 - accuracy: 0.9457\n",
      "Epoch 73/150\n",
      "59/59 [==============================] - 0s 965us/step - loss: 0.1364 - accuracy: 0.9461\n",
      "Epoch 74/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1369 - accuracy: 0.9460\n",
      "Epoch 75/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1369 - accuracy: 0.9450\n",
      "Epoch 76/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1353 - accuracy: 0.9465\n",
      "Epoch 77/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1362 - accuracy: 0.9462\n",
      "Epoch 78/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9466\n",
      "Epoch 79/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9463\n",
      "Epoch 80/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9461\n",
      "Epoch 81/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1347 - accuracy: 0.9462\n",
      "Epoch 82/150\n",
      "59/59 [==============================] - 0s 1000us/step - loss: 0.1364 - accuracy: 0.9454\n",
      "Epoch 83/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9457\n",
      "Epoch 84/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9466\n",
      "Epoch 85/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9461\n",
      "Epoch 86/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9470\n",
      "Epoch 87/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1377 - accuracy: 0.9445\n",
      "Epoch 88/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1352 - accuracy: 0.9457\n",
      "Epoch 89/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1328 - accuracy: 0.9469\n",
      "Epoch 90/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1340 - accuracy: 0.9459\n",
      "Epoch 91/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1342 - accuracy: 0.9464\n",
      "Epoch 92/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1322 - accuracy: 0.9474\n",
      "Epoch 93/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1326 - accuracy: 0.9468\n",
      "Epoch 94/150\n",
      "59/59 [==============================] - 0s 845us/step - loss: 0.1320 - accuracy: 0.9468\n",
      "Epoch 95/150\n",
      "59/59 [==============================] - 0s 983us/step - loss: 0.1345 - accuracy: 0.9451\n",
      "Epoch 96/150\n",
      "59/59 [==============================] - 0s 983us/step - loss: 0.1318 - accuracy: 0.9480\n",
      "Epoch 97/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1316 - accuracy: 0.9473\n",
      "Epoch 98/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1327 - accuracy: 0.9462\n",
      "Epoch 99/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1329 - accuracy: 0.9466\n",
      "Epoch 100/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1336 - accuracy: 0.9461\n",
      "Epoch 101/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1324 - accuracy: 0.9461\n",
      "Epoch 102/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1319 - accuracy: 0.9466\n",
      "Epoch 103/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9477\n",
      "Epoch 104/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9464\n",
      "Epoch 105/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9465\n",
      "Epoch 106/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9466\n",
      "Epoch 107/150\n",
      "59/59 [==============================] - 0s 960us/step - loss: 0.1315 - accuracy: 0.9468\n",
      "Epoch 108/150\n",
      "59/59 [==============================] - 0s 954us/step - loss: 0.1341 - accuracy: 0.9458\n",
      "Epoch 109/150\n",
      "59/59 [==============================] - 0s 930us/step - loss: 0.1315 - accuracy: 0.9466\n",
      "Epoch 110/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1309 - accuracy: 0.9474\n",
      "Epoch 111/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1314 - accuracy: 0.9467\n",
      "Epoch 112/150\n",
      "59/59 [==============================] - 0s 947us/step - loss: 0.1296 - accuracy: 0.9475\n",
      "Epoch 113/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1320 - accuracy: 0.9459\n",
      "Epoch 114/150\n",
      "59/59 [==============================] - 0s 965us/step - loss: 0.1290 - accuracy: 0.9482\n",
      "Epoch 115/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9477\n",
      "Epoch 116/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9472\n",
      "Epoch 117/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9477\n",
      "Epoch 118/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1296 - accuracy: 0.9465\n",
      "Epoch 119/150\n",
      "59/59 [==============================] - 0s 966us/step - loss: 0.1310 - accuracy: 0.9465\n",
      "Epoch 120/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1300 - accuracy: 0.9470\n",
      "Epoch 121/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1283 - accuracy: 0.9485\n",
      "Epoch 122/150\n",
      "59/59 [==============================] - 0s 915us/step - loss: 0.1299 - accuracy: 0.9475\n",
      "Epoch 123/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1311 - accuracy: 0.9469\n",
      "Epoch 124/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1302 - accuracy: 0.9470\n",
      "Epoch 125/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1286 - accuracy: 0.9479\n",
      "Epoch 126/150\n",
      "59/59 [==============================] - 0s 900us/step - loss: 0.1288 - accuracy: 0.9474\n",
      "Epoch 127/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1299 - accuracy: 0.9468\n",
      "Epoch 128/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1295 - accuracy: 0.9472\n",
      "Epoch 129/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1270 - accuracy: 0.9484\n",
      "Epoch 130/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1279 - accuracy: 0.9475\n",
      "Epoch 131/150\n",
      "59/59 [==============================] - 0s 911us/step - loss: 0.1277 - accuracy: 0.9483\n",
      "Epoch 132/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1274 - accuracy: 0.9485\n",
      "Epoch 133/150\n",
      "59/59 [==============================] - 0s 884us/step - loss: 0.1295 - accuracy: 0.9474\n",
      "Epoch 134/150\n",
      "59/59 [==============================] - 0s 910us/step - loss: 0.1282 - accuracy: 0.9480\n",
      "Epoch 135/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9471\n",
      "Epoch 136/150\n",
      "59/59 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9484\n",
      "Epoch 137/150\n",
      "59/59 [==============================] - 0s 931us/step - loss: 0.1266 - accuracy: 0.9488\n",
      "Epoch 138/150\n",
      "59/59 [==============================] - 0s 879us/step - loss: 0.1261 - accuracy: 0.9484\n",
      "Epoch 139/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1290 - accuracy: 0.9473\n",
      "Epoch 140/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1274 - accuracy: 0.9480\n",
      "Epoch 141/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1276 - accuracy: 0.9489\n",
      "Epoch 142/150\n",
      "59/59 [==============================] - 0s 983us/step - loss: 0.1249 - accuracy: 0.9495\n",
      "Epoch 143/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1262 - accuracy: 0.9484\n",
      "Epoch 144/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1271 - accuracy: 0.9478\n",
      "Epoch 145/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1250 - accuracy: 0.9493\n",
      "Epoch 146/150\n",
      "59/59 [==============================] - 0s 862us/step - loss: 0.1277 - accuracy: 0.9480\n",
      "Epoch 147/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1255 - accuracy: 0.9494\n",
      "Epoch 148/150\n",
      "59/59 [==============================] - 0s 897us/step - loss: 0.1253 - accuracy: 0.9493\n",
      "Epoch 149/150\n",
      "59/59 [==============================] - 0s 948us/step - loss: 0.1251 - accuracy: 0.9494\n",
      "Epoch 150/150\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.1261 - accuracy: 0.9491\n"
     ]
    }
   ],
   "source": [
    "adam = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=1024, epochs=150) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loss curve using the same function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be evaluated on the test set with familiar code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9501\n",
      "F1: 0.6928\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.array(y_pred > 0.5, dtype=int).squeeze()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy)\n",
    "print(\"F1: %.4f\" % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
