{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADML MEP Script\n",
    "##### Scripts for calculating specific ML tasks! Dave B. HSLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9782608695652174\n",
      "0.6126916290641964\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "a = [3, 14, 18, 23]\n",
    "b = [12, 16, 21, 29]\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)\n",
    "\n",
    "a = [0.6,0,1,0.1]\n",
    "b = [0.8,0.1,0.1,0.2]\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance\n",
    "point_1 = (22,18,286)\n",
    "point_2 = (183,1,350)\n",
    "def naive_euclidian_distance(point1, point2):\n",
    "    differences = [point1[x] - point2[x] for x in range(len(point1))]\n",
    "    differences_squared = [difference ** 2 for difference in differences]\n",
    "    sum_of_squares = sum(differences_squared)\n",
    "    return sum_of_squares ** 0.5\n",
    "print(naive_euclidian_distance(point_1, point_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "sigmoid(71-70)+sigmoid(0-70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Regression model\n",
    "# read data\n",
    "df_house = pd.read_csv(\"kc_house_data.csv\")\n",
    "# select 2 features\n",
    "df_house = df_house[[\"price\",\"sqft_living\"]]\n",
    "# split data in train and test\n",
    "train_house, test_house = train_test_split(df_house, test_size=0.8, random_state=42)\n",
    "# normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_house = pd.DataFrame(scaler.fit_transform(train_house), columns=train_house.columns, index=train_house.index)\n",
    "test_house = pd.DataFrame(scaler.transform(test_house), columns=test_house.columns, index=test_house.index)\n",
    "# split data in x and y\n",
    "X_train_house = train_house[[\"sqft_living\"]]\n",
    "y_train_house = train_house[[\"price\"]]\n",
    "\n",
    "X_test_house = test_house[[\"sqft_living\"]]\n",
    "y_test_house = test_house[[\"price\"]]\n",
    "\n",
    "def fit(X, y):\n",
    "    thetas = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return thetas\n",
    "thetas = fit(X_train_house, y_train_house)\n",
    "# predict\n",
    "def predict(X, thetas):\n",
    "    y_pred = np.dot(X, thetas)\n",
    "    return y_pred\n",
    "y_pred_house = predict(X_train_house, thetas)\n",
    "y_pred_house\n",
    "\n",
    "# check performance with test-data\n",
    "y_pred_test_house = predict(X_test_house, thetas)\n",
    "r2_house = r2_score(y_test_house, y_pred_test_house)\n",
    "print(\"R2: \", r2_house)\n",
    "\n",
    "## Adding polynomial feature\n",
    "#copying data\n",
    "X_train_deg2 = X_train_house.copy()\n",
    "X_train_deg2[\"sqft_living**2\"] = X_train_deg2[\"sqft_living\"] * X_train_deg2[\"sqft_living\"]\n",
    "\n",
    "X_test_deg2 = X_test_house.copy()\n",
    "X_test_deg2[\"sqft_living^2\"] = X_test_deg2[\"sqft_living\"] * X_test_deg2[\"sqft_living\"]\n",
    "\n",
    "# Fit the model\n",
    "thetas_deg2 = fit(X_train_deg2, y_train_house)\n",
    "# check performance\n",
    "y_pred_test_deg2 = predict(X_test_deg2, thetas_deg2)\n",
    "r2_house_deg2 = r2_score(y_test_house, y_pred_test_deg2)\n",
    "print(\"R2: \", r2_house_deg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy and Sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 80\n",
    "FP = 10\n",
    "TN = 500\n",
    "FN = 20\n",
    "print(\"Confusion Matrix\")\n",
    "print('''          | Predicted\n",
    "Act       | NO    | YES   |\n",
    "ua    NO  | {}    | {}    |\n",
    "l     Yes | {}    | {}    |\n",
    "      '''.format(TN, FP, FN, TP))\n",
    "\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "accuracy = (TN+TP)/(TN+TP+FN+FP)\n",
    "precision = TP/(TP+FP)\n",
    "F1 = (2*precision*sensitivity)/(precision+sensitivity)\n",
    "print('''\n",
    "      sensitivity: {}\n",
    "      specificity: {}\n",
    "      accuracy: {}\n",
    "      precision: {}\n",
    "      F1: {}\n",
    "      '''.format(sensitivity, specificity, accuracy, precision,F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# generate data\n",
    "students_passed = np.random.normal(5,0.7,100)\n",
    "students_failed = np.random.normal(2,0.7,100)\n",
    "zeros = [0]*100\n",
    "ones = [1]*100\n",
    "X = np.concatenate((students_passed, students_failed))\n",
    "y = np.concatenate((ones, zeros))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "x = np.linspace(-8, 8)\n",
    "plt.plot(x, sigmoid(x))\n",
    "\n",
    "\n",
    "def predict(X, theta0, theta1):\n",
    "    z = theta0 +  theta1 * X\n",
    "    y_pred = sigmoid(z)\n",
    "    return y_pred\n",
    "\n",
    "theta0 = 1.0\n",
    "theta1 = 1.0\n",
    "\n",
    "y_pred = predict(X, theta0, theta1)\n",
    "\n",
    "# cost function\n",
    "def cost_function(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    cost = -(1.0 / n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    return cost\n",
    "\n",
    "# for applying gradient descent, we define the gradient.\n",
    "def gradient(X, y, theta0, theta1):\n",
    "    y_pred = predict(X, theta0, theta1)\n",
    "    diff = y_pred - y\n",
    "    \n",
    "    n = len(X)\n",
    "    grad_theta0 = np.sum(diff) / n\n",
    "    grad_theta1 = np.dot(diff, X.T) / n\n",
    "    \n",
    "    return grad_theta0, grad_theta1\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "# Now we are ready to determine the optimal values for the parameters and using the gradient descent algorithm.\n",
    "def fit(X, y, alpha, num_epochs, display_every=10):\n",
    "    theta0 = 0.0\n",
    "    theta1 = np.random.randn()\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        # calculate gradients\n",
    "        grad_theta0, grad_theta1 = gradient(X, y, theta0, theta1)\n",
    "        \n",
    "        # update model parameters theta0 and theta1\n",
    "        theta0 = theta0 - alpha * grad_theta0\n",
    "        theta1 = theta1 - alpha * grad_theta1\n",
    "        \n",
    "        # calculate the current costs\n",
    "        y_pred = predict(X, theta0, theta1)\n",
    "        curr_cost = cost_function(y, y_pred)\n",
    "        \n",
    "        hist[\"cost\"].append(curr_cost)\n",
    "        hist[\"theta0\"].append(theta0)\n",
    "        hist[\"theta1\"].append(theta1)\n",
    "        \n",
    "        if i == 0 or (i+1) % display_every == 0:\n",
    "            print(\"Epoch {} -  cost: {}\".format(i+1, curr_cost))\n",
    "        \n",
    "    return theta0, theta1, hist\n",
    "\n",
    "# plot functions\n",
    "def plot_validation_curve(costs, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Cost\")\n",
    "    ax.set_title(\"Validation Curve\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.plot(costs)\n",
    "\n",
    "def plot_decision_boundary(X, theta0, theta1, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    x = np.arange(X.min()-1, X.max()+1, 0.01).reshape(-1,1)\n",
    "    y_pred = predict(x, theta0, theta1)\n",
    "    ax.plot(x, y_pred, color=\"r\")\n",
    "    ax.axvline(-theta0/theta1, color=\"g\")\n",
    "    ax.set_title(\"Decision Boundary\")\n",
    "    \n",
    "legend_map = {0: 'failed', 1: 'passed'}\n",
    "ax = sns.scatterplot(X, y, hue=pd.Series(y).map(legend_map))\n",
    "ax.set_xlabel('days spent learning for the ML exam')\n",
    "ax.set_ylabel('if students passed')\n",
    "plot_decision_boundary(X, theta0, theta1, ax)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Run gradient descent algorithm\n",
    "# Let's run the gradient descent algorithm!\n",
    "alpha = 0.1\n",
    "num_epochs = 100000\n",
    "\n",
    "theta0, theta1, hist = fit(X, y, alpha, num_epochs, display_every=1000)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,4))\n",
    "\n",
    "# scatter plot\n",
    "legend_map = {0: 'failed', 1: 'passed'}\n",
    "ax[0] = sns.scatterplot(X, y, hue=pd.Series(y).map(legend_map), ax=ax[0])\n",
    "ax[0].set_xlabel('days spent learning for the ML exam')\n",
    "ax[0].set_ylabel('if students passed')\n",
    "plot_decision_boundary(X, theta0, theta1, ax[0])\n",
    "\n",
    "# validation curve\n",
    "plot_validation_curve(hist[\"cost\"], ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the normal vector w \n",
    "steigung von W berechnen und anschliessen geteilt durch länge rechnen\n",
    "anschliessend punkt auf gerade bestimmen\n",
    "nacher \n",
    "https://www.mycloud.swisscom.ch/s/S0051A2360516F854AA2D2130132540B416B31BA207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "gini total: 0.5333333333333334\n"
     ]
    }
   ],
   "source": [
    "#Calculate Gini Impurity of Feature \"Color\"\n",
    "#------------------------------------------#\n",
    "#red\n",
    "r_fast = 0\n",
    "r_shelf = 1\n",
    "r_normal =  3\n",
    "r_sum = r_fast+r_shelf+r_normal\n",
    "r_gini = 1- ((r_fast/r_sum)**2 + (r_shelf/r_sum)**2 + (r_normal/r_sum)**2)\n",
    "#white\n",
    "w_fast = 2\n",
    "w_shelf = 2\n",
    "w_normal = 2\n",
    "w_sum = w_fast+w_shelf+w_normal\n",
    "w_gini = 1- ((w_fast/w_sum)**2 + (w_shelf/w_sum)**2 + (w_normal/w_sum)**2)\n",
    "#silver\n",
    "s_fast = 2\n",
    "s_shelf = 1\n",
    "s_normal = 1\n",
    "s_sum = s_fast+s_shelf+s_normal\n",
    "s_gini = 1- ((s_fast/s_sum)**2 + (s_shelf/s_sum)**2 + (s_normal/s_sum)**2)\n",
    "#black\n",
    "b_fast = 4\n",
    "b_shelf = 2\n",
    "b_normal = 0\n",
    "b_sum = b_fast+b_shelf+b_normal\n",
    "b_gini = 1- ((b_fast/b_sum)**2 + (b_shelf/b_sum)**2 + (b_normal/b_sum)**2)\n",
    "\n",
    "total = r_sum+w_sum+s_sum+b_sum\n",
    "print(total)\n",
    "\n",
    "print(\"gini total: \" + str(r_gini*(r_sum/total)+(w_gini*(w_sum/total)+(s_gini*(s_sum/total))+(b_gini*(b_sum/total)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0.0\n",
      "0.4444444444444444\n",
      "0.6428571428571429\n",
      "gini total: 0.5166666666666667\n"
     ]
    }
   ],
   "source": [
    "#Calculate Gini Impurity of Feature \"Seats\". The feature should be split into three categories:\n",
    "\n",
    "\n",
    "\n",
    "# one   -> \"Seats < 3\"\n",
    "one_fast = 3\n",
    "one_shelf = 0\n",
    "one_normal = 0\n",
    "one_sum = one_fast+one_shelf+one_normal\n",
    "one_gini = 1- ((one_fast/one_sum)**2 + (one_shelf/one_sum)**2 +(one_normal/one_sum)**2 )\n",
    "\n",
    "# two   -> \"Seats ≥ 3 and Seats < 4.5\"\n",
    "two_fast = 2\n",
    "two_shelf = 1\n",
    "two_normal = 0\n",
    "two_sum = two_fast+two_shelf+two_normal\n",
    "two_gini = 1- ((two_fast/two_sum)**2 + (two_shelf/two_sum)**2 +(two_normal/two_sum)**2 )\n",
    "\n",
    "# three ->\"Seats ≥ 4.5\"\n",
    "three_fast = 3\n",
    "three_shelf = 5\n",
    "three_normal = 6\n",
    "three_sum = three_fast+three_shelf+three_normal\n",
    "three_gini = 1- ((three_fast/three_sum)**2 + (three_shelf/three_sum)**2 +(three_normal/three_sum)**2 )\n",
    "\n",
    "\n",
    "total = one_sum+two_sum+three_sum\n",
    "print(total)\n",
    "print(one_gini)\n",
    "print(two_gini)\n",
    "print(three_gini)\n",
    "\n",
    "print(\"gini total: \" + str(one_gini*(one_sum/total) + (two_gini*(two_sum/total) + (three_gini*(three_sum/total)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "gini total: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "#Calculate Gini Impurity of Color after Seats split for the node Seats ≥ 4.5\n",
    "#red\n",
    "r_fast = 0\n",
    "r_shelf = 1\n",
    "r_normal =  3\n",
    "r_sum = r_fast+r_shelf+r_normal\n",
    "r_gini = 1- ((r_fast/r_sum)**2 + (r_shelf/r_sum)**2 + (r_normal/r_sum)**2)\n",
    "#white\n",
    "w_fast = 0\n",
    "w_shelf = 2\n",
    "w_normal = 2\n",
    "w_sum = w_fast+w_shelf+w_normal\n",
    "w_gini = 1- ((w_fast/w_sum)**2 + (w_shelf/w_sum)**2 + (w_normal/w_sum)**2)\n",
    "#silver\n",
    "s_fast = 0\n",
    "s_shelf = 1\n",
    "s_normal = 1\n",
    "s_sum = s_fast+s_shelf+s_normal\n",
    "s_gini = 1- ((s_fast/s_sum)**2 + (s_shelf/s_sum)**2 + (s_normal/s_sum)**2)\n",
    "#black\n",
    "b_fast = 3\n",
    "b_shelf = 1\n",
    "b_normal = 0\n",
    "b_sum = b_fast+b_shelf+b_normal\n",
    "b_gini = 1- ((b_fast/b_sum)**2 + (b_shelf/b_sum)**2 + (b_normal/b_sum)**2)\n",
    "\n",
    "total = r_sum+w_sum+s_sum+b_sum\n",
    "print(total)\n",
    "\n",
    "print(\"gini total: \" + str(r_gini*(r_sum/total)+(w_gini*(w_sum/total)+(s_gini*(s_sum/total))+(b_gini*(b_sum/total)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support: {'lego': 0.75, 'oats': 0.75, 'doll': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from ctypes import sizeof\n",
    "from re import I\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "transactions = [['oats', 'lego', 'teddybear', 'rc car'],\n",
    "                ['oats', 'red coat', 'gloves', 'teddybear', 'doll', 'warm boot'],\n",
    "                ['lego', 'red jelly bag cap', 'rc car', 'doll'],\n",
    "                ['lego', 'oats', 'large red bag', 'gift wrap paper', 'warm boot']]\n",
    "transactions = pd.DataFrame(data={\"Items\":transactions}, index=range(1,5))\n",
    "transactions.index.name = 'Id'\n",
    "\n",
    "# Support\n",
    "support = {}\n",
    "for item in ['lego', 'oats', 'doll']:\n",
    "    support[item] = transactions.Items.map(lambda x: item in x).sum() / transactions.shape[0] # support of 'lego'\n",
    "print(\"Support: \" + str(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence\n",
    "#Calculate the confidence of ['lego', 'oats'] -> ['teddybear']\n",
    "XUY = ['lego', 'oats','teddybear']\n",
    "\n",
    "XUY_frequency = 0.25\n",
    "# confidence_lego_oats-teddybear = 0.25 / 0.5\n",
    "confidence = XUY_frequency / ( transactions.Items.map(lambda x: 'lego' in x and 'oats' in x).sum() / transactions.shape[0]  )\n",
    "print(\"Confidence is: \" + str(confidence))\n",
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support           itemsets\n",
      "0     0.50             (doll)\n",
      "1     0.75             (lego)\n",
      "2     0.75             (oats)\n",
      "3     0.50           (rc car)\n",
      "4     0.50        (teddybear)\n",
      "5     0.50        (warm boot)\n",
      "6     0.50       (oats, lego)\n",
      "7     0.50     (lego, rc car)\n",
      "8     0.50  (oats, teddybear)\n",
      "9     0.50  (oats, warm boot)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(rc car)</td>\n",
       "      <td>(lego)</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(teddybear)</td>\n",
       "      <td>(oats)</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(warm boot)</td>\n",
       "      <td>(oats)</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents consequents  antecedent support  consequent support  support  \\\n",
       "0     (rc car)      (lego)                 0.5                0.75      0.5   \n",
       "1  (teddybear)      (oats)                 0.5                0.75      0.5   \n",
       "2  (warm boot)      (oats)                 0.5                0.75      0.5   \n",
       "\n",
       "   confidence      lift  leverage  conviction  \n",
       "0         1.0  1.333333     0.125         inf  \n",
       "1         1.0  1.333333     0.125         inf  \n",
       "2         1.0  1.333333     0.125         inf  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apriori Algorithm\n",
    "#minimum support of 0.5 and minimum confidence of 0.75\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(transactions.Items.values.tolist())\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(apriori(df,min_support=0.5,use_colnames=True,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Execute the following code to show the solution. We will see how to use this library in a minute.\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(transactions.Items.values.tolist())\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "freq_itemsets = apriori(df, use_colnames=True, min_support=0.5)\n",
    "freq_itemsets\n",
    "# Execute the following code to show the solution\n",
    "association_rules(freq_itemsets, metric='confidence', min_threshold=0.75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 3., 2.]),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Eigenvalues and Eigenvectors\n",
    "a = np.array([[1,0,0],[0,3,0], [0,0, 2]])\n",
    "np.linalg.eig(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8fe2f68fb170d3e59715732971b3e459b293954a8edfc2b4e8212f6d304449f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
